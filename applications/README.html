<html>
<head>
</head>
<!-- saved from url=(0014)about:internet -->
<body>
<h1>License</h1>
<p>Copyright (c) 2017-2022 Software AG, Darmstadt, Germany and/or Software AG USA Inc., Reston, VA, USA, and/or its subsidiaries and/or its affiliates and/or their licensors.  </p>
<p>Licensed under the Apache License, Version 2.0 (the "License"); you may not use this
file except in compliance with the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the
License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
either express or implied. 
See the License for the specific language governing permissions and limitations under the License.</p>
<h1>Using containers for Apama applications</h1>
<p>The samples in this directory demonstrate how you can use Docker or Kubernetes to deploy and
manage entire applications.</p>
<p>An application will typically consist of multiple interacting processes. It is
best-practice in the Docker world to containerize only individual processes, 
and to link them together for network connections and file access.</p>
<p>While all of this can be accomplished by sequences of Docker or Kubernetes commands such as
'build' and 'run', Both Docker and Kubernetes provide methods to automates some
of this through configuration files. We demonstrate how to use it for
deploying and managing entire applications in the samples provided.</p>
<p>Furthermore, the containers in an Apama application will typically need more
than the base image. The base image provides Apama executables, but these
executables will need access to configuration files or EPL in order to be
actually useful in an application. These samples also contain Dockerfiles that
create derived images on top of the base image, copying in the necessary
files.</p>
<p>The most educational way to go through the sample applications is in this
order:</p>
<p>| Sample              | Description |
|---------------------|-------------|
| Simple/             | Injecting EPL into a correlator |
| Adapter/            | Connecting a correlator and the IAF (not available with Apama Core)|
| Weather/            | Running Ant-exported projects and generating dashboards (not available with Apama Core)|
| MemoryStore/        | Using volumes for persistent state across rebuilds|
| UniversalMessaging/ | Using UM for communication between two Apama correlators (not available with Apama Core)|
| Secrets/            | Using Docker and Kubernetes secrets to pass encrypted data to the correlator through config files|</p>
<h1>Docker Compose</h1>
<p>Docker Compose is invoked using the 'docker-compose' tool, which is documented
at http://docs.docker.com/compose/. You should already have 'docker-compose'
installed and available on your PATH. These samples drive the tool through the
use of configuration files called 'docker-compose.yml', the format of which is
documented at http://docs.docker.com/compose/yml/ .</p>
<h1>Docker Stack</h1>
<p>Additionally to Docker Compose, docker stack can be used to get levels of scalability and resource
management. Docker stack https://docs.docker.com/engine/reference/commandline/stack/
uses the docker-compose.yml files with added specific entries detailing how to deploy
the containers.</p>
<p>Prior to running docker stack samples the daemon being used should be enabled by an administrator
using docker swarm init. https://docs.docker.com/engine/reference/commandline/swarm_init/</p>
<h1>Kubernetes</h1>
<p>As an alternative to Docker Compose and stack, each sample also contains Kubernetes
configuration files usually called 'kubernetes.yml'. This format is documented at
https://kubernetes.io/docs/reference/ These can be used to deploy to any Kubernetes cluster.</p>
<h1>Running the samples using Docker Compose</h1>
<p>Each sample contains a README with information specific to that sample,
describing what it does and how to interact with the containers that it
creates. The 'docker-compose.yml' configuration file in each sample is also
documented to describe the purpose and rationale of each element.</p>
<p>The 'cheat sheet' section below provides even more detail about the elements
of Compose configuration that we use.</p>
<p>Before running a sample, check the README for any extra prerequisites and
instructions. Each sample can be invoked by navigating into the sample's
directory below this one, and invoking Compose to bring the sample up:</p>
<pre><code>&gt; docker-compose up -d
</code></pre>
<p>Compose will then pull or build the necessary images for each service
described in the 'docker-compose.yml' configuration file. It will then bring
up containers for each of them, configuring and linking them together as
specified.</p>
<p>You can then inspect the logs of each container:</p>
<pre><code>&gt; docker-compose logs
</code></pre>
<p>The output will be streamed to your console, colour-coded to distinguish each
container.</p>
<p>When you are done, stop and remove all containers launched by Compose:</p>
<pre><code>&gt; docker-compose stop
&gt; docker-compose rm
</code></pre>
<p>You may wish to experiment with other 'docker' and 'docker-compose' commands
to investigate the containers and images created by Compose.</p>
<h1>Running the samples with Kubernetes</h1>
<p>All the samples can be worked through with Kubernetes in the same order as
above.</p>
<p>To run the samples using Kubernetes you first have to build the sample images
using docker build in a docker host from the directory of that sample:</p>
<pre><code>&gt; docker build -t imagename .
</code></pre>
<p>Or, if the sample has multiple images:</p>
<pre><code>&gt; docker build -t image1name dir1
&gt; docker build -t image2name dir2
</code></pre>
<p>Then you must tag and push the images to a registry so that they can be
retrieved by the Kubernetes server:</p>
<pre><code>&gt; docker tag imagename registry/org/repository:image
&gt; docker push registry/org/repository:image
</code></pre>
<p>Lastly you must modify the provided kubernetes.yml to have the registry tag of
the new image in the place of the various '-image's. Now you are able to create
the deployment in Kubernetes:</p>
<pre><code>&gt; kubectl create -f kubernetes.yml
</code></pre>
<p>The kubernetes.yml will name the pod that you've created. This name will be
used to inspect, stop and delete the pod. Replace 'sample' in the following
commands with the name of the pod for a given sample. </p>
<p>To inspect the logs from the containers in the pod:</p>
<pre><code>&gt; kubectl logs sample
</code></pre>
<p>To remove the deployment:</p>
<pre><code>&gt; kubectl delete -f kubernetes.yml
</code></pre>
<h1>docker-compose.yml cheat sheet</h1>
<p>'services:'</p>
<p>This describes what services should be created for this Composition and is a
top-level configuration.</p>
<p>'image:'</p>
<p>Refers to an already-existing image that should be used to create the
container.</p>
<p>'build:'</p>
<p>Used in place of 'image:', the build value points to a directory to build into
an image, based on a Dockerfile located there. In our samples this is
typically used to build new images containing configuration or EPL specific to
that sample.</p>
<p>'networks:'</p>
<p>This can be used in 2 places.  At the top-level, it describes what networks
Compose should create.  When used in a service, it says which networks the
services container should connect to.  Containers on the same network to
discover and connect to each other using the services name as a hostname.
This allows for separation of different containers onto different networks.</p>
<p>See https://docs.docker.com/compose/networking/</p>
<p>'volumes:'</p>
<p>This can be used in 2 places.  At the top-level, it describes what volumes
Compose should create.  When used in a service, it says which volumes the
services container should mount and use.  A volume will appear in a
container's filesystem at the path specified.  Volumes can be shared between
containers and are persisted between container restarts.  This is used by
the 'MemoryStore' sample for giving the correlator a persisted database.</p>
<p>See https://docs.docker.com/engine/userguide/containers/dockervolumes/</p>
<p>'ports:'</p>
<p>Makes a listening port in the container accessible to the host operating
system, on a potentially different port. e.g. a ports entry like '8080:80'
means that anything in the host operating system connecting to localhost:8080
will get connected to a process listening on port 80 inside the container. Our
samples show that you should only do this for containers that need to be
accessible from the outside world. All networking that is purely
inter-container should remain private.</p>
<p>'depends_on:'</p>
<p>Express dependency between services to ensure start ordering.</p>
<h1>Next steps</h1>
<p>After exploring these samples, you might wish to create your own Docker-ized
applications, extending the existing samples and/or using the comprehensive
documentation available on Docker's website (http://docs.docker.com/). Here
are some ideas to get started:</p>
<ul>
<li>
<p>Try exporting one of your own Apama projects using Ant Export and turn it
  into an application in Docker. You should be able to modify the existing
  Weather sample, which is itself using a project that was exported using Ant
  Export.</p>
</li>
<li>
<p>Suppose you have a long-running application using a correlator and an IAF.
  You wish to reconfigure the IAF without restarting the correlator and thus
  losing application state. You could accomplish this using the options to the
  'docker-compose' tool that allow you to rebuild and restart individual
  containers in a running Compose environment.</p>
</li>
</ul>
<h1>Changes from previous releases</h1>
<h2>Additional Secrets Sample</h2>
<p>Demonstrates use of Docker and Kubernetes secrets to pass encrypted data
to the correlator through config files.</p>
<h2>Version 3.3 of Compose YAML files</h2>
<p>This gives us access to much more Compose functionality (such as network,
volumes, etc) and Version 1 is deprecated and expected to be removed at
some point.</p>
<h2>Deployment Containers</h2>
<p>We have moved away from using deployment containers (where a short lived
container would inject code into a separate container running the correlator).
We have moved away from using engine_inject on the command line (which also
required using engine_management --waitFor to wait for the correlator to
start).</p>
<p>We now use the --config command line argument to the correlator to specify an
init.yaml configuration file which lists what files (.mon, .evt, .cdp or .jar)
to inject at startup. This is good practice and allows for use with Docker
Swarm and to easily scale distributed applications.</p>
<h2>Links</h2>
<p>We no longer use container 'links', instead using networking to connect
container together.  'links' are deprecated and do not work on multi-host
Swarms.</p>
<h2>Volumes</h2>
<p>Volumes are now first class Compose citizens and we no longer need to
create dedicated data volume containers.</p>
<h2>Kubernetes configuration</h2>
<p>Added Kubernetes configuration for each sample.</p>
</body>
</html>